---
type: publication

title: "The challenge of understanding explanations for user trust in AI: Insights from an experiment about job matching"
subtitle: ''
summary: ''
authors:
- Glenda Hannibal
- Christine Bauer
tags: [JobMatcher, job recommendation, recommender systems, trust, AI, explanations]
categories: []
date: 2026-01-01
featured: false
draft: false
profile: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
#image:
#  caption: ''
#  focal_point: ''
#  preview_only: false


projects: []
publishDate: '2025-09-08T01:13:45.989615Z'
publication_types: [paper-conference]
publication: '*20th IFIP TC13 International Conference on Human-Computer Interaction*'
publication_short: INTERACT 2025

abstract: "Artificial Intelligence (AI) is widely used in recruitment with Job Recommender Systems (JRS) being designed for job matching. However, poor matches can significantly impact individuals' livelihoods, business profits, and organizational productivity. Thus, it is critical to study whether users trust JRS and their outputs, especially after receiving explanations for job matches. In a between-subjects, mixed-methods online experiment, we study whether varying explanations of trust violations embedded in a job match influence user trust in the mock-up JRS algorithm JobMatcher. We found that such explanations had limited effect on trust in the JobMatcher algorithm or understanding of the job match. We discuss the findings about the complexity of explanations concerning trust, the problematic role of high or low agency perception of AI for trust measures, and the need to address the moral dimension of trust."

altmetric: false
plumx: false
dimensions: false
doi: 10.1007/978-3-032-05002-1_33
#url_pdf: 
#links:
#- icon: file
#  icon_pack: fa
#  name: Postprint
#  url: 
---
