---
title: "Dagstuhl Perspectives Workshop 24352: Conversational Agents: A Framework for Evaluation (CAFE)"
cms_exclude: true
type: talks

draft: false
reading_time: false  # Show estimated reading time?
share: true  # Show social sharing links?
profile: false  # Show author profile?
commentable: false  # Allow visitors to comment? Supported by the Page, Post, and Docs content types.
editable: false  # Allow visitors to edit the page? Supported by the Page, Post, and Docs content types.

event: "Dagstuhl Perspectives Workshop 24352: Conversational Agents: A Framework for Evaluation (CAFE)"
event_url: "https://www.dagstuhl.de/24352"

location: Schloss Dagstuhl
address:
  street: Oktavie-Allee
  city: Wadern
#  region: CA
  postcode: '66687'
  country: Germany

summary: "Dagstuhl Perspectives Workshop 24352: Conversational Agents: A Framework for Evaluation (CAFE)"
abstract: |-
  *Conversational Agents* (CA) as frontends to *Information Retrieval* (IR) and *Recommender Systems* (RS) become more popular in everyday life, with a wider range of users and usages. The latest developments in *Large Language Models* (LLMs) will have tremendous consequences, especially for the workplace and education. In this Dagstuhl Perspectives Workshop, we want to focus on the evaluation of these conversational systems, as appropriate methods are still missing. The quality of these systems is limited in terms of personalization, veracity and correctness, bias, transparency, trustworthiness, and understandability. Thus, evaluation methods must address these shortcomings. Furthermore, user- and usage-oriented aspects should become a more prominent and integral component in evaluations, as the user population as well as the tasks these systems are used for become more heterogeneous. For this reason, the topic-centric view of relevance has to be extended to a broad range of facets which are important for the different usage scenarios. Therefore, suitable evaluation criteria have to be specified, which form the basis for defining appropriate measures. Most importantly, the range of evaluation methods must be revisited and extended, as popular methods like the Cranfield approach or crowdsourcing must be complemented by new evaluation methods and strategies specifically tailored to this new type of system.

  More in detail, we will focus our discussion on several key open issues, among which are the following:
  - how to cross the borders of different areas, mainly Information Retrieval and Recommender Systems in our case, but also Natural Language Processing; 
  - how to create experimental collections and evaluate Large Language Models in terms of their bias, explainability, veracity, correctness, and hallucination in the CA context;  
  -  how to incorporate user- and usage-oriented facets in order to understand how users’ perceived conversation qualities (e.g., attentiveness, adaptability, understanding, and response quality) and perceived recommendation qualities (e.g.,, accuracy, novelty, interaction adequacy, and explanation) might interact with each other in a CA to affect user beliefs (e.g., perceived usefulness, perceived ease of use, transparency, user control, rapport, humanness), user attitudes (e.g., user satisfaction, trust), and behavioral intentions (e.g., intention to use);   
  - how to measure information leakage and privacy, and how to ensure that a CA does not propagate sensitive information;   
  -  how to devise proper simulation approaches to support both the development and the evaluation of a CA, avoiding circularity (the techniques used for simulation are similar to those used for developing systems), ensuring reliability, and reducing the gap between offline measurements and online user evaluations;    
  - how to evaluate to what extent answers/recommendations produced by a CA are appropriate, tailored to, and understandable for a specific audience, e.g., school kids, the general public, professionals, and people with (cognitive) disabilities.
    
  Overall, all the above questions call, as one possible output of the workshop, for envisioning *some reference architecture for CA systems, geared towards evaluation*, which allows the different areas to cooperate on a common ground and to share a common roadmap for improving our understanding of CA systems and making them more effective.

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: "2024-08-25"
date_end: "2024-08-30"
all_day: true

# Schedule page publish date (NOT talk date).
publishDate: "2023-01-01T00:00:00Z"

authors:
  - Christine Bauer
  - Li Chen
  - Nicola Ferro
  - Norbert Fuhr
tags: [Dagstuhl, evaluation, information access, information retrieval, recommender systems, NLP, conversational agents, LLM]
categories:
  - event

# Is this a featured talk? (true/false)
featured: false

image:
  caption: 'Dagstuhl Perspectives Workshop 24352: Conversational Agents: A Framework for Evaluation (CAFE)<br>
  (Aug 25 – Aug 30, 2024)'
  focal_point: "Smart"
  preview_only: false
  placement: 1


links:
#url_code: ""
#url_pdf: ""
#url_slides: ""
#url_video: ""
#url_proceedings: ''
#doi: 


projects: [multimethod]
---

{{< cite page="bauer2025_dagstuhl_cafe_report" view="4" >}}
